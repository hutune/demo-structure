# Mono repo Golang 마이크로서비스 팀

Kafka 기반 Golang 마이크로서비스를 Kubernetes와 Claude Code로 개발한다면, \*\*모노레포가 명확한 정답\*\*입니다. 조율 오버헤드 감소, Kafka 스키마 공유 간소화, AI 기반 개발 효율성, 그리고 Uber와 Monzo처럼 수천 개의 Go 마이크로서비스를 모노레포로 성공적으로 운영하는 기업들의 사례가 이를 뒷받침합니다. 반면 Segment의 멀티레포 마이크로서비스 실패 사례는 경고가 됩니다.

독립 배포와 느슨한 결합이라는 요구사항은 적절한 구조만 갖추면 모노레포에서 완벽히 달성 가능합니다. 핵심 통찰: \*\*레포지토리 구조가 서비스 결합도를 결정하는 것이 아니라, 코드 설계가 결정합니다.\*\*

\---

\## 소규모 Go 팀에 모노레포가 유리한 이유

1-2명 개발자 팀에 대한 전문가들의 의견은 명확합니다. Ricardo Soares(LinkedIn)는 이렇게 말합니다: "회사 규모가 매우 작을 때 모노레포 접근법이 가장 역동적입니다. 멀티레포는 일반적으로 소규모 팀에게 훨씬 더 비용이 많이 듭니다." Earthly의 Brandon Schurman도 동의합니다: "소규모 팀과 일할 때 모노레포가 특히 매력적입니다."

\*\*Go 워크스페이스([go.work](http://go.work))가 판도를 바꿉니다\*\*. Go 1.18에서 도입된 워크스페이스는 모노레포에서 여러 모듈을 관리하는 마찰을 제거합니다. 각 서비스는 자체 \`go.mod\`를 유지하지만, \`go.work\`를 통해 모든 서비스를 동시에 개발할 수 있습니다. 공유 라이브러리 변경사항이 즉시 반영되고, 개발 중 \`replace\` 지시문이 필요 없으며, gopls가 모듈 간 탐색과 리팩토링을 지원합니다.

소규모 팀에서 멀티레포의 진짜 문제는 \*\*조율 오버헤드\*\*입니다. 멀티레포에서 여러 서비스에 걸친 변경은: 라이브러리 레포 업데이트 → PR → 머지 → 태그 릴리즈 → service-1에서 의존성 버전 업 → PR → 머지 → 각 서비스마다 반복. 모노레포에서는? 단일 원자적 커밋으로 끝납니다. Segment는 이를 뼈저리게 경험했습니다—120개 이상의 레포로 "3명의 풀타임 엔지니어가 시스템을 살려두는 데만 대부분의 시간을 소비했습니다."

| 요소 | 모노레포 (1-2명) | 멀티레포 (1-2명) |
|------|-----------------|-----------------|
| 서비스 간 리팩토링 | 단일 커밋 | 여러 개의 조율된 PR |
| 공유 라이브러리 업데이트 | 즉시 반영 | 태그 → 배포 → 각 소비자 버전업 |
| CI/CD 유지보수 | 하나의 파이프라인 | N개의 파이프라인 관리 |
| 코드 리뷰 컨텍스트 | 완전한 가시성 | 레포별로 분산 |
| 인지 부하 | 하나의 멘탈 모델 | 어떤 레포에 무엇이 있는지 추적 필요 |

\---

\## Kafka 이벤트 스키마 공유: 모노레포가 모든 것을 단순화

Kafka 기반 이벤트 드리븐 아키텍처에서 스키마 관리는 핵심입니다—그리고 모노레포에서 훨씬 단순해집니다. 서비스들이 이벤트 계약을 공유한다면, 스키마 정의도 공유해야 합니다.

\*\*권장 접근법: 모노레포에서 Protobuf + Buf CLI 사용\*\*

\`\`\`
monorepo/
├── proto/
│ ├── buf.yaml # Buf 설정
│ ├── buf.gen.yaml # 코드 생성 설정
│ ├── common/
│ │ └── types.proto # 공유 타입 (타임스탬프, ID)
│ └── events/
│ ├── order/v1/events.proto
│ └── user/v1/events.proto
├── generated/
│ └── go/ # 생성된 Go 코드
├── services/
│ └── \[모든 서비스가 generated/go 임포트\]
\`\`\`

\`buf generate\`를 실행하면 Go 구조체가 생성됩니다. 모든 서비스는 \`[github.com/yourorg/monorepo/generated/go/events\`](http://github.com/yourorg/monorepo/generated/go/events`)를 임포트합니다. 스키마 변경과 소비 코드가 하나의 커밋으로 원자적으로 업데이트됩니다—버전 조율이 필요 없습니다.

\*\*런타임 검증\*\*을 위해서는 Confluent Schema Registry를 \*\*BACKWARD\*\* 호환성 모드로 사용하세요. 프로덕션용 주요 설정:

\- \`auto.register.schemas=false\` (자동 등록 비활성화)
\- \`normalize.schemas=true\` (스키마 정규화 활성화)
\- 주제 명명에 TopicNameStrategy 사용: \`{topic}-value\`

멀티레포 대안—Go 모듈로 배포되는 전용 스키마 레포—은 상당한 오버헤드를 추가합니다: 스키마 변경 시 라이브러리를 배포한 후, 모든 소비 서비스에서 버전을 업해야 합니다. 이것이 바로 Segment의 마이크로서비스 붕괴를 야기한 패턴입니다.

\---

\## 모노레포에서 독립 배포 달성하기

서비스별 독립 배포는 GitHub Actions의 \*\*경로 기반 트리거\*\*를 사용해 모노레포에서 완벽히 달성 가능합니다. 이것은 타협이 아닙니다—Uber와 Monzo가 사용하는 표준 패턴입니다.

\*\*서비스별 GitHub Actions 워크플로우:\*\*

\`\`\`yaml
name: user-service
on:
push:
paths:
\- 'services/user-service/\*\*'
\- 'pkg/\*\*' _\# 공유 라이브러리_
\- 'proto/events/user/\*\*' _\# 서비스별 스키마_
branches: \[main\]

jobs:
build-deploy:
runs-on: ubuntu-latest
steps:
\- uses: actions/checkout@v4
\- uses: actions/setup-go@v5
with:
go-version: '1.23'
cache-dependency-path: 'services/user-service/go.sum'
\- name: Test
run: cd services/user-service && go test ./...
\- name: Build and push
run: |
docker build -t [ghcr.io/$](http://ghcr.io/$){{ github.repository }}/user-service:${{ github.sha }} \\
\-f services/user-service/Dockerfile .
docker push [ghcr.io/$](http://ghcr.io/$){{ github.repository }}/user-service:${{ github.sha }}
\`\`\`

10개 미만 서비스의 소규모 팀이라면, \*\*\`git diff\`를 사용한 커스텀 스크립트\*\*로 영향받는 서비스를 감지하면 충분합니다—Bazel이나 Nx는 필요 없습니다. 서비스가 늘어나면 템플릿에서 워크플로우 파일을 생성하세요.

\---

\## GitOps를 통한 Kubernetes 매니페스트 구성

\*\*권장 구조: 서비스별 Helm 차트 + ArgoCD ApplicationSets\*\*

\`\`\`
monorepo/
├── services/
│ ├── user-service/
│ │ ├── src/
│ │ └── chart/ # 서비스별 Helm 차트
│ │ ├── Chart.yaml
│ │ ├── values.yaml # 기본값
│ │ ├── values-dev.yaml # Dev 오버라이드
│ │ ├── values-stg.yaml # Staging 오버라이드
│ │ ├── values-prd.yaml # Production 오버라이드
│ │ └── templates/
├── infrastructure/
│ ├── base/ # 공유 K8s 리소스
│ └── overlays/
│ ├── dev/
│ ├── stg/
│ └── prd/
└── argocd/
└── appset.yaml # 모든 서비스용 ApplicationSet
\`\`\`

\*\*ArgoCD ApplicationSet\*\*이 서비스를 동적으로 발견합니다:

\`\`\`yaml
apiVersion: [argoproj.io/v1alpha1](http://argoproj.io/v1alpha1)
kind: ApplicationSet
metadata:
name: microservices
spec:
goTemplate: true
generators:
\- git:
repoURL: [https://github.com/org/monorepo.git](https://github.com/org/monorepo.git)
revision: HEAD
directories:
\- path: services/\*/chart
template:
metadata:
name: '{{index .path.segments 1}}-{{.values.env}}'
spec:
project: default
source:
repoURL: [https://github.com/org/monorepo.git](https://github.com/org/monorepo.git)
path: '{{.path.path}}'
helm:
valueFiles:
\- values-{{.values.env}}.yaml
destination:
server: [https://kubernetes.default.svc](https://kubernetes.default.svc)
namespace: '{{index .path.segments 1}}'
\`\`\`

이 패턴을 사용하면 새 서비스 추가 시 차트가 포함된 디렉토리만 생성하면 됩니다—ArgoCD가 자동으로 발견합니다.

\---

\## Go 중심 기업들의 실제 검증 사례

수천 개의 Go 마이크로서비스를 성공적으로 운영하는 기업들은 모두 모노레포를 사용합니다:

\*\*Uber\*\*: 3,000개 이상의 마이크로서비스, 약 5천만 줄의 Go 코드, Go 모노레포에서 하루 1,000건 이상의 커밋. 2018년에 의존성 관리 문제로 멀티레포에서 모노레포로 마이그레이션했습니다.

\*\*Monzo\*\*: 2,800개 이상의 마이크로서비스, 모두 Go, 모두 하나의 모노레포. 플랫폼 엔지니어 Will Sewell: "모노레포를 갖는 것은 마이크로서비스와 일반적으로 연관된 일부 도전들을 관리하는 데 엄청나게 도움이 됩니다." 일관된 의존성 버전, 서비스 간 컴파일 타임 피드백, 대규모 리팩토링 역량을 핵심 이점으로 꼽습니다.

\*\*Segment의 교훈\*\*: 모놀리스로 시작해서, 120개 이상의 별도 레포로 마이크로서비스로 전환했다가, 2017년에 \*\*모놀리스로 다시 돌아갔습니다\*\*. 엔지니어들은 "공유 라이브러리 지옥"을 묘사했습니다—서비스마다 다른 버전, 테스트 악몽, 서비스 수에 비례해 증가하는 운영 오버헤드. "더 빠르게 움직일 수 있게 해주는 대신, 소규모 팀은 폭발적인 복잡성에 빠져들었습니다."

패턴은 명확합니다: 대규모로 마이크로서비스에 성공한 기업들은 모노레포를 사용합니다. 멀티레포 마이크로서비스로 어려움을 겪은 기업들은 다시 전환하거나 인프라 팀에 대규모 투자를 했습니다—1-2명 팀에서는 실현 불가능합니다.

\---

\## Claude Code는 모노레포에서 훨씬 더 잘 작동합니다

AI 기반 개발에서 \*\*컨텍스트가 결정적 요소\*\*입니다—그리고 모노레포는 멀티레포 설정이 상당한 엔지니어링 없이는 제공할 수 없는 고유한 컨텍스트 이점을 제공합니다.

\*\*모노레포의 Claude Code 이점:\*\*
\- 서비스가 공유 라이브러리와 어떻게 상호작용하는지 완전한 가시성
\- 단일 계층적 [CLAUDE.md](http://CLAUDE.md) 구조가 완전한 컨텍스트 제공
\- 서비스 간 리팩토링을 전체 컨텍스트에서 이해
\- 한 개발자는 모노레포의 계층적 구조를 활용해 CLAUDE.md를 \*\*47k에서 9k 단어로\*\* 줄였습니다

\*\*멀티레포의 문제점\*\*: 팀들이 크로스레포 컨텍스트 중복으로 \*\*토큰 예산의 40-60%를 소비\*\*한다고 보고합니다. Claude가 하나의 레포에 있으면, "다른 서비스의 API가 어떻게 생겼는지 전혀 모릅니다."

\*\*권장 [CLAUDE.md](http://CLAUDE.md) 구조:\*\*

\`\`\`
monorepo/
├── [CLAUDE.md](http://CLAUDE.md) # 루트: 아키텍처 개요, 공통 명령어
├── services/
│ ├── user-service/
│ │ └── [CLAUDE.md](http://CLAUDE.md) # 서비스별: API 계약, 의존성
│ └── order-service/
│ └── [CLAUDE.md](http://CLAUDE.md)
├── proto/
│ └── [CLAUDE.md](http://CLAUDE.md) # 스키마 컨벤션, 생성 명령어
└── pkg/
└── [CLAUDE.md](http://CLAUDE.md) # 공유 라이브러리 패턴
\`\`\`

루트 [CLAUDE.md](http://CLAUDE.md) 예시 내용:
\`\`\`markdown
\# 플랫폼 모노레포

\## 빠른 명령어
\- 전체 빌드: \`make build\`
\- 서비스 테스트: \`cd services/<n> && go test ./...\`
\- Proto 생성: \`buf generate\`
\- 스테이징 배포: \`make deploy-stg SERVICE=<n>\`

\## 아키텍처
\- 서비스는 Kafka 이벤트로 통신 (proto/events/ 참조)
\- 공유 라이브러리는 pkg/에 위치 - 내부 패키지로 임포트
\- 각 서비스는 chart/ 하위 디렉토리에 독립 Helm 차트 보유

\## 컨벤션
\- 이벤트 명명: PascalCase 동사 (OrderCreated, UserUpdated)
\- 서비스 디렉토리는 Kubernetes 네임스페이스 이름과 일치
\`\`\`

\---

\## RMN 프로젝트를 위한 권장 모노레포 구조

모든 리서치를 바탕으로, Golang 마이크로서비스 + Flutter + Kafka + Kubernetes에 최적화된 구조입니다:

\`\`\`
platform/
├── [CLAUDE.md](http://CLAUDE.md) # AI 컨텍스트: 아키텍처, 명령어
├── [go.work](http://go.work) # Go 워크스페이스 (gitignore)
├── buf.yaml # Protobuf 설정
├── buf.gen.yaml
│
├── proto/ # 이벤트 스키마
│ └── events/
│ ├── order/v1/
│ └── user/v1/
├── generated/ # 생성된 코드 (커밋됨)
│ └── go/
│
├── services/ # Go 마이크로서비스
│ ├── api-gateway/
│ │ ├── go.mod
│ │ ├── main.go
│ │ ├── Dockerfile
│ │ └── chart/ # Helm 차트
│ │ ├── Chart.yaml
│ │ ├── values.yaml
│ │ ├── values-dev.yaml
│ │ ├── values-stg.yaml
│ │ └── values-prd.yaml
│ ├── user-service/
│ └── order-service/
│
├── pkg/ # 공유 Go 라이브러리
│ ├── kafka/ # Kafka 클라이언트 래퍼
│ ├── middleware/ # 공통 HTTP/gRPC 미들웨어
│ └── config/ # 설정 로딩
│
├── web/ # Flutter 웹
│ └── flutter\_app/
│
├── infrastructure/ # 공유 K8s 리소스
│ ├── kafka/
│ └── monitoring/
│
├── argocd/ # GitOps 설정
│ └── appset.yaml
│
├── .github/
│ └── workflows/
│ ├── user-service.yaml # 서비스별 CI
│ ├── order-service.yaml
│ └── flutter-web.yaml
│
└── Makefile # 공통 명령어
\`\`\`

\---

\## 요약: 요구사항별 결정 매트릭스

| 요구사항 | 모노레포 솔루션 | 멀티레포에서 필요한 것 |
|---------|---------------|---------------------|
| \*\*1-2명 개발자\*\* | 자연스러운 적합 - 최소한의 조율 | 레포 관리에 상당한 오버헤드 |
| \*\*독립 배포\*\* | 경로 기반 GitHub Actions 트리거 | 이미 독립적 (하지만 더 많은 파이프라인) |
| \*\*느슨한 결합\*\* | 코드 설계로 결정, 레포 구조 아님 | 동일한 규율 필요 |
| \*\*Kafka 스키마 공유\*\* | 단일 proto/ 디렉토리, 원자적 업데이트 | 전용 스키마 레포 + 버전 관리 |
| \*\*K8s (dev/stg/prd)\*\* | 레포 내 환경별 Helm values | 어느 쪽이든 동일한 복잡성 |
| \*\*Claude Code\*\* | 전체 컨텍스트, 계층적 [CLAUDE.md](http://CLAUDE.md) | 컨텍스트에 40-60% 토큰 오버헤드 |
| \*\*GitHub 워크플로우\*\* | 단일 레포 검색, 통합 PR | 크로스레포 조율 필요 |

\*\*최종 권장\*\*: 위 구조로 모노레포를 채택하세요. 로컬 개발에는 Go 워크스페이스를, Protobuf 관리에는 Buf를, 독립 배포에는 경로 기반 GitHub Actions를, GitOps에는 ArgoCD ApplicationSets를 사용하세요. 소규모 팀 규모는 실제로 모노레포의 가장 강력한 사용 사례입니다—모든 이점(원자적 커밋, 공유 코드, 통합 도구)을 얻으면서 단점(Bazel이 필요한 규모 문제)은 최소화됩니다.

서비스가 밀접하게 연관되어 있다면 루트에 단일 \`go.mod\`로 시작하고, 더 명확한 경계를 원한다면 서비스별 \`go.mod\`와 \`go.work\`를 사용하세요. 둘 다 작동합니다; 후자가 나중에 서비스를 추출해야 할 때 더 유연합니다. 핵심 통찰: \*\*느슨한 결합은 인터페이스 설계와 이벤트 계약을 통해 달성되지, 레포지토리 분리로 달성되지 않습니다.\*\*